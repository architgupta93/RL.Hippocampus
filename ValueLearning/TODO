[PENDING]
1.  Something funny is going on with the weights when we are learning a
    specific task. Plot a histogram of the weights and see how they are
    distributed at different points in learning. This has direct implications
    to how well multiple environments can be learnt.

2.  In case of multiple environments with very few steps allowed in each
    environment in a given alternation, critic learns different value functions
    pretty easily. However, the actor is not able to learn the correct weights to
    make a differentiation in actions for the two different environments.

3.  Sometimes, the values encoded by the critic also blow up dramatically.
    What is going on here. It just occured to me that value iteration does
    take a lot longer to converge than policy iteration. And policy iteration
    does give you the correct 'behavior'. Stopping well before values
    saturate is certainly a good idea in this case.

4.  Y coordinate of the goal location is off-by-one. Need to check!

[DONE]
1.  Start with random initial values for the critic instead of starting with
    all zeros.

2.  Add a momentum term to the 'Actor' agent. Instead of relying only on the
    current activity, it has a memory of the past activities and incorporates
    that in making a decision.

3.  Plot the trajectories with the same x-y limits to make it easier to
    visualize the trajectories. Also add markers for the start and end
    points.