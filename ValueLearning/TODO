[PENDING]
1.  Need different learning rates for actor and critic.

2.  Adding walls to the maze: debugging needed (Only for edge cases - Probably
    when you start on one of the walls - still need to debug the wall's
    'includes' function)

3.  Visualization: Separately plot the goal location. Also add functionality
    to plot the walls for the new environments.

4.  Y coordinate of the goal location is off-by-one. Need to check!

5.  If a trial ends in 0 iterations (Agent starts very close to the goal
    location), the value function is not updated at all. This needs to be fixed.

[DONE]
1.  Start with random initial values for the critic instead of starting with
    all zeros.

2.  Add a momentum term to the 'Actor' agent. Instead of relying only on the
    current activity, it has a memory of the past activities and incorporates
    that in making a decision.

3.  Plot the trajectories with the same x-y limits to make it easier to
    visualize the trajectories. Also add markers for the start and end
    points.

4.  Something funny is going on with the weights when we are learning a
    specific task. Plot a histogram of the weights and see how they are
    distributed at different points in learning. This has direct implications
    to how well multiple environments can be learnt.

5.  In case of multiple environments with very few steps allowed in each
    environment in a given alternation, critic learns different value functions
    pretty easily. However, the actor is not able to learn the correct weights to
    make a differentiation in actions for the two different environments.